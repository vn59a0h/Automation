{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "5e5af204",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "64691bd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>icdsTableName</th>\n",
       "      <th>BANNER_NAME</th>\n",
       "      <th>dataSensitivity</th>\n",
       "      <th>OP-Company code</th>\n",
       "      <th>dlSchemaName</th>\n",
       "      <th>dlTableName</th>\n",
       "      <th>tableLoadType</th>\n",
       "      <th>keyPreCombine</th>\n",
       "      <th>keyPrimaryKey</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A071</td>\n",
       "      <td>MDD,MAK,MSB,MM,WMT</td>\n",
       "      <td>se</td>\n",
       "      <td>SA-MDD,SA-MAK,SA-MSB</td>\n",
       "      <td>sa_mdse_dl_secure</td>\n",
       "      <td>site_article_info</td>\n",
       "      <td>incremental</td>\n",
       "      <td>ds_load_ts</td>\n",
       "      <td>clnt,application,cond_type,sales_org,distr_cha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A073</td>\n",
       "      <td>MDD,MAK</td>\n",
       "      <td>se</td>\n",
       "      <td>SA-MDD,SA-MAK</td>\n",
       "      <td>sa_mdse_dl_secure</td>\n",
       "      <td>site_article_info</td>\n",
       "      <td>incremental</td>\n",
       "      <td>ds_load_ts</td>\n",
       "      <td>clnt,application,cond_type,sales_org,distr_cha...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  icdsTableName         BANNER_NAME dataSensitivity       OP-Company code  \\\n",
       "0          A071  MDD,MAK,MSB,MM,WMT              se  SA-MDD,SA-MAK,SA-MSB   \n",
       "1          A073             MDD,MAK              se         SA-MDD,SA-MAK   \n",
       "\n",
       "        dlSchemaName        dlTableName tableLoadType keyPreCombine  \\\n",
       "0  sa_mdse_dl_secure  site_article_info   incremental    ds_load_ts   \n",
       "1  sa_mdse_dl_secure  site_article_info   incremental    ds_load_ts   \n",
       "\n",
       "                                       keyPrimaryKey  \n",
       "0  clnt,application,cond_type,sales_org,distr_cha...  \n",
       "1  clnt,application,cond_type,sales_org,distr_cha...  "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = f\"/Users/vn59a0h/Desktop/projects/Ingestion/Automation/ingestion.csv\"\n",
    "df = pd.read_csv(filename)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e82f3e0",
   "metadata": {},
   "source": [
    "## MODULE 1 : DAG PREPARATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77d8a54",
   "metadata": {},
   "source": [
    "### 1.1 Generate DAG Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "dd679f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dag_name(banner, tableLoadType, dlSchemaName, dlTableName):\n",
    "    \"\"\"Generate formatted script string from input parameters.\"\"\"\n",
    "    if isinstance(banner, list):\n",
    "        results = []\n",
    "        for b in banner:\n",
    "            results.append(f\"INTLDLDAT-SA{b}-{tableLoadType}-{dlSchemaName.upper()}-{b}_{dlTableName}\")\n",
    "        return results\n",
    "    return f\"INTLDLDAT-SA{banner}-{tableLoadType}-{dlSchemaName.upper()}-{banner}_{dlTableName}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c1788e",
   "metadata": {},
   "source": [
    "### 1.2 Prepare DAG File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941e1b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import json\n",
    "from airflow import models\n",
    "from airflow.models import Variable\n",
    "from airflow.exceptions import AirflowException\n",
    "from airflow.utils.dates import days_ago\n",
    "from airflow.providers.google.cloud.operators.dataproc import DataprocSubmitJobOperator, DataprocDeleteClusterOperator, DataprocCreateClusterOperator\n",
    "from airflow.operators.bash_operator import BashOperator\n",
    "from airflow.contrib.hooks.gcs_hook import GoogleCloudStorageHook\n",
    "from datetime import datetime,timedelta\n",
    "import logging\n",
    "import pendulum\n",
    "from plugins.CustomModule import  on_failure_spotlight, send_p1_email\n",
    "import pytz\n",
    "from airflow.exceptions import AirflowFailException\n",
    "from airflow.models import Param\n",
    "from bfdms.dpaas import BFDMSDataprocCreateClusterOperator\n",
    "from airflow.operators.empty import EmptyOperator\n",
    "\n",
    "# Task 1. Cluster creation\n",
    "# b. Ingestion from ICDS\n",
    "#         \n",
    "# Task 3. Upsert Target\n",
    "# Task 4. Cluster deletion\n",
    "\n",
    "SENSITIVITY=\"SE\"\n",
    "PRIORITY=\"P2\"\n",
    "TAGS = [\"Massmart-eComm\",PRIORITY,\"Ephemeral\",\"SA\",\"SECURE\",\"MDSE\",\"MAKRO\",\"mak_physl_invt_doc\",\"SLT\"]\n",
    "CLUSTER_NAME = \"sa-mdse-dl-secure-mak-physl-invt-doc\"\n",
    "ARTIFACTORY_URL = Variable.get(\"ARTIFACTORY_URL\")\n",
    "BANNER_NAME=\"makro\"\n",
    "TRUE_FLAG = \"true\"\n",
    "TABLE_NAME=\"IKPF\" #SAP table name\n",
    "\n",
    "\n",
    "# Dynamic cluster configuration parameters\n",
    "machine_type = \"{{ params.machineType }}\"\n",
    "\n",
    "ing_driver_cores = \"{{ params.ingDriverCores }}\"\n",
    "ing_driver_memory = \"{{ params.ingDriverMemory }}\"\n",
    "ing_exec_instances = \"{{ params.ingExecInstances }}\"\n",
    "ing_exec_cores = \"{{ params.ingExecCores }}\"\n",
    "ing_exec_memory = \"{{ params.ingExecMemory }}\"\n",
    "ing_memory_overhead = \"{{ params.ingMemoryOverhead }}\"\n",
    "\n",
    "ups_driver_cores = \"{{ params.upsDriverCores }}\"\n",
    "ups_driver_memory = \"{{ params.upsDriverMemory }}\"\n",
    "ups_exec_instances = \"{{ params.upsExecInstances }}\"\n",
    "ups_exec_cores = \"{{ params.upsExecCores }}\"\n",
    "ups_exec_memory = \"{{ params.upsExecMemory }}\"\n",
    "ups_memory_overhead = \"{{ params.upsMemoryOverhead }}\"\n",
    "cluster_type = \"{{ params.clusterType }}\"\n",
    "\n",
    "ALERT_EMAIL_ADDRESSES = ['intltechdatamassmart@email.wal-mart.com']\n",
    "BQ_SYNC = TRUE_FLAG\n",
    "CCM_URL = Variable.get(\"CCM_URL\")\n",
    "DAG_ID = os.path.basename(__file__).replace(\".pyc\", \"\").replace(\".py\", \"\")\n",
    "ENV = Variable.get(\"ENV\")\n",
    "\n",
    "SCHEDULE = \"\"\n",
    "if \"DEV\" in ENV:\n",
    "    SCHEDULE = None\n",
    "elif \"PROD\" in ENV:\n",
    "    SCHEDULE=\"0 22 * * *\"  # Schedule updated to run daily at 11:30 PM\n",
    "\n",
    "dag_arr = DAG_ID.split('-')\n",
    "if len(dag_arr) == 5:\n",
    "    load_type = dag_arr[2].lower()\n",
    "    target_schema = dag_arr[1][0:2].lower()+'_'+dag_arr[3].lower()\n",
    "    target_table = dag_arr[4].lower()\n",
    "else:\n",
    "    raise AirflowException(\"Please make sure DAG name is in right format:<INTLDLDAT>-<DIVISION:SAWM>-<LOAD_TYPE: INC,FULL>-<TARGET_SCHEMA>-<TARGET_TABLE>\")\n",
    "\n",
    "# **************************** Read Global bucket property file *****************************\n",
    "def read_properties(gcs_file):\n",
    "    try:\n",
    "        property_file=\"\"\n",
    "        if \"non-prod\" in CCM_URL:\n",
    "            property_file=gcs_hook.download(GCS_CODE_BUCKET, gcs_file, filename=gcs_file.split(\"/\")[-1])\n",
    "        elif \"prod\" in CCM_URL:\n",
    "            property_file='/usr/local/airflow/'+gcs_file\n",
    "        props = {}\n",
    "        with open(property_file,'r') as datafile:\n",
    "            for line in datafile:\n",
    "                if (len(line) > 1 and '=' in line):\n",
    "                    key, value = line.split(\"=\")\n",
    "                    props[key.lower()] = value.strip('\\n')\n",
    "        return props\n",
    "    except (FileNotFoundError, IOError):\n",
    "        print(\"Global Property File is not found in local at: \",property_file)\n",
    "\n",
    "# **************************** Get Cluster Configurations *****************************\n",
    "def read_cluster_config():\n",
    "    try:\n",
    "        file_bytes = ('/usr/local/airflow/'+CLUSTER_CONFIG_FILE) if(ENV=='PROD') else (gcs_hook.download(GCS_CODE_BUCKET, CLUSTER_CONFIG_FILE, filename=CLUSTER_CONFIG_FILE.split(\"/\")[-1]))\n",
    "\n",
    "        with open(file_bytes, 'r') as datafile:\n",
    "            data = json.load(datafile)\n",
    "            CLUSTER_CONFIG = data[CLUSTER_TYPE]\n",
    "            CLUSTER_CONFIG['dpaas_env'] = SENSITIVITY.lower()\n",
    "            CLUSTER_CONFIG['cluster_config']['gce_cluster_config']['service_account'] = SERVICE_ACCOUNT\n",
    "            CLUSTER_CONFIG['cluster_config']['gce_cluster_config']['metadata']['startup-script-url'] = 'gs://{}/{}'.format(GCS_CODE_BUCKET, init_actions)\n",
    "\n",
    "            # Dynamic cluster configuration for custom cluster type\n",
    "            if CLUSTER_TYPE == 'custom':\n",
    "                CLUSTER_CONFIG['cluster_config']['master_config']['machine_type_uri'] = machine_type\n",
    "                CLUSTER_CONFIG['cluster_config']['worker_config']['machine_type_uri'] = machine_type\n",
    "                CLUSTER_CONFIG['cluster_config']['secondary_worker_config']['machine_type_uri'] = machine_type\n",
    "                # You can make these dynamic as well if needed\n",
    "                CLUSTER_CONFIG['cluster_config']['worker_config']['num_instances'] = 2\n",
    "                CLUSTER_CONFIG['cluster_config']['secondary_worker_config']['num_instances'] = 0\n",
    "\n",
    "        return CLUSTER_CONFIG\n",
    "    except (FileNotFoundError, IOError):\n",
    "        print(\"Failed to download or read cluster config file: \",file_bytes)\n",
    "\n",
    "GCS_CODE_BUCKET = Variable.get(\"GCS_CODE_BUCKET\")\n",
    "CONN_ID_DPAAS = Variable.get(\"CONN_ID_\"+SENSITIVITY+\"_DPAAS\")\n",
    "SOFTWARE_CONFIGS = Variable.get(\"SOFTWARE_CONFIG_\"+SENSITIVITY)\n",
    "\n",
    "# Provide GoogleCloud Platform connection ID\n",
    "gcs_hook = GoogleCloudStorageHook(gcp_conn_id=CONN_ID_DPAAS, delegate_to=None)\n",
    "global_prop_file = 'configs/global/'+SENSITIVITY.lower()+'_bucket_dpaas.properties'\n",
    "global_props = read_properties(global_prop_file)\n",
    "team_space = global_props['team_space']\n",
    "dpaas_env = global_props['dpaas_env']\n",
    "REGION = global_props['region']\n",
    "PROJECT_ID = global_props['project_id']\n",
    "EMAIL=global_props['email']\n",
    "SERVICE_ACCOUNT = global_props['service_account_'+SENSITIVITY.lower()]\n",
    "logging.info(f\"REGION: {REGION}, PROJECT_ID: {PROJECT_ID}, SERVICE_ACCOUNT: {SERVICE_ACCOUNT}\")\n",
    "\n",
    "CLUSTER_CONFIG_FILE = \"configs/cluster_config/dpaas_cluster_create.json\"\n",
    "init_actions = global_props['init_actions']\n",
    "CLUSTER_TYPE = 'custom'  # Changed from 'medium' to 'custom' for dynamic configuration\n",
    "CLUSTER_CONFIG = read_cluster_config()\n",
    "logging.info(f\"CLUSTER_CONFIG: {CLUSTER_CONFIG}\")\n",
    "\n",
    "\n",
    "def failure_callback(context):\n",
    "    if \"prod\" in PROJECT_ID.lower():\n",
    "        if PRIORITY == \"P1\":\n",
    "            send_p1_email(context,PROJECT_ID,EMAIL)\n",
    "        on_failure_spotlight(context,PROJECT_ID,REGION)\n",
    "\n",
    "default_args = {\n",
    "    'owner': 'walmart',\n",
    "    'depends_on_past': False,\n",
    "    'email_on_failure': True,\n",
    "    'email': ALERT_EMAIL_ADDRESSES,\n",
    "    'email_on_retry': False,\n",
    "    'retries': 1,\n",
    "    'sla': timedelta(minutes=60),\n",
    "    'retry_delay': timedelta(minutes=3),\n",
    "    'max_active_runs':1,\n",
    "    'on_failure_callback': failure_callback,\n",
    "}\n",
    "\n",
    "\n",
    "# *************************************** Spark Job - Ingestion from SAP Source ************************************************\n",
    "\n",
    "ICDS_INGESTION_MAIN_CLASS = \"za.co.massmart.icds.ds.IngestionPipeline\"\n",
    "\n",
    "JARS_FILES = [\n",
    "               f\"gs://{GCS_CODE_BUCKET}/utilities/hudi_jar/hudi-spark3.3-bundle_2.12-1.0.0.jar\",\n",
    "               f\"gs://{GCS_CODE_BUCKET}/utilities/hudi_jar/hudi-hadoop-mr-bundle-1.0.0.jar\", # Required for creating Hive external table using org.apache.hudi.hadoop.HoodieParquetInputFormat data format\n",
    "               f\"gs://{GCS_CODE_BUCKET}/utilities/hudi_jar/google-http-client-jackson2-1.43.1.jar\",\n",
    "               f\"gs://{GCS_CODE_BUCKET}/utilities/hudi_jar/google-cloud-bigquery-2.24.4.jar\",\n",
    "               f\"gs://{GCS_CODE_BUCKET}/utilities/hudi_jar/hudi-gcp-bundle-1.0.0.jar\",\n",
    "               f\"gs://{GCS_CODE_BUCKET}/utilities/common_jars/mssql-jdbc-12.2.0.jre8.jar\",\n",
    "               f\"gs://{GCS_CODE_BUCKET}/utilities/app_jars/icds-ds.jar\"\n",
    "               ]\n",
    "\n",
    "ICDS_INGESTION_JOB_NAME = \"ICDS_To_DataLake_Raw_Ingestion\"\n",
    "ICDS_INGESTION_CMD_LINE_ARGS = [\"--banner\", BANNER_NAME, \"--sapTableName\", TABLE_NAME, \"--dpaasFlag\", TRUE_FLAG]\n",
    "\n",
    "ICDS_INGESTION_SPARK_PROP = {\n",
    "    \"spark.app.name\": ICDS_INGESTION_JOB_NAME,\n",
    "    \"spark.master\": \"yarn\",\n",
    "    \"spark.submit.deployMode\": \"client\",\n",
    "\n",
    "    # Dynamic Spark configuration for ingestion\n",
    "    \"spark.driver.cores\": ing_driver_cores,\n",
    "    \"spark.driver.memory\": ing_driver_memory,\n",
    "    \"spark.driver.memoryOverheadFactor\": ing_memory_overhead,\n",
    "    \"spark.executor.instances\": ing_exec_instances,\n",
    "    \"spark.executor.cores\": ing_exec_cores,\n",
    "    \"spark.executor.memory\": ing_exec_memory,\n",
    "    \"spark.executor.memoryOverheadFactor\": ing_memory_overhead,\n",
    "\n",
    "    \"spark.task.maxFailures\": \"3\",\n",
    "    \"spark.executor.extraJavaOptions\": \"-Dsun.net.http.allowRestrictedHeaders=true\",\n",
    "    \"spark.jars.repositories\": ARTIFACTORY_URL,\n",
    "    \"spark.driver.extraJavaOptions\": f\"-Druntime.context.system.property.override.enabled=true -Druntime.context.environmentType=lab -Dscm.server.url={CCM_URL} -Dcom.walmart.platform.metrics.logfile.path=/dev/null -Dcom.walmart.platform.logging.logfile.path=/dev/null -Dcom.walmart.platform.txnmarking.logfile.path=/dev/null\",\n",
    "    \"spark.yarn.maxAppAttempts\": \"1\"\n",
    "}\n",
    "\n",
    "ICDS_INGESTION_SPARK_JOB = {\n",
    "    \"reference\": {\"project_id\": PROJECT_ID},\n",
    "    \"placement\": {\"cluster_name\": CLUSTER_NAME},\n",
    "    \"spark_job\": {\n",
    "        \"main_class\": ICDS_INGESTION_MAIN_CLASS,\n",
    "        \"args\": ICDS_INGESTION_CMD_LINE_ARGS,\n",
    "        \"jar_file_uris\": JARS_FILES,\n",
    "        \"properties\": ICDS_INGESTION_SPARK_PROP\n",
    "    },\n",
    "}\n",
    "\n",
    "# *************************************** Spark Job - Upsert with Target Table ************************************************\n",
    "\n",
    "UPSERT_TARGET_MAIN_CLASS = \"za.co.massmart.icds.ds.UpsertPipelineV1\"\n",
    "\n",
    "UPSERT_TARGET_JOB_NAME = \"Upsert_Target_DataLake_Table\"\n",
    "\n",
    "UPSERT_TARGET_CMD_LINE_ARGS = [\"--banner\", BANNER_NAME, \"--sapTableName\", TABLE_NAME,\"--bqSync\",BQ_SYNC]\n",
    "\n",
    "UPSERT_TARGET_SPARK_PROP = {\n",
    "    \"spark.app.name\": UPSERT_TARGET_JOB_NAME,\n",
    "    \"spark.master\": \"yarn\",\n",
    "    \"spark.submit.deployMode\": \"client\",\n",
    "\n",
    "    # Dynamic Spark configuration for upsert\n",
    "    \"spark.driver.cores\": ups_driver_cores,\n",
    "    \"spark.driver.memory\": ups_driver_memory,\n",
    "    \"spark.driver.memoryOverheadFactor\": ups_memory_overhead,\n",
    "    \"spark.executor.instances\": ups_exec_instances,\n",
    "    \"spark.executor.cores\": ups_exec_cores,\n",
    "    \"spark.executor.memory\": ups_exec_memory,\n",
    "    \"spark.executor.memoryOverheadFactor\": ups_memory_overhead,\n",
    "\n",
    "    \"spark.task.maxFailures\": \"3\",\n",
    "    \"spark.executor.extraJavaOptions\": \"-Dsun.net.http.allowRestrictedHeaders=true\",\n",
    "    \"spark.jars.repositories\": ARTIFACTORY_URL,\n",
    "    \"spark.driver.extraJavaOptions\": f\"-Druntime.context.system.property.override.enabled=true -Druntime.context.environmentType=lab -Dscm.server.url={CCM_URL} -Dcom.walmart.platform.metrics.logfile.path=/dev/null -Dcom.walmart.platform.logging.logfile.path=/dev/null -Dcom.walmart.platform.txnmarking.logfile.path=/dev/null\",\n",
    "    \"spark.yarn.maxAppAttempts\": \"1\"\n",
    "}\n",
    "\n",
    "UPSERT_TARGET_SPARK_JOB = {\n",
    "    \"reference\": {\"project_id\": PROJECT_ID},\n",
    "    \"placement\": {\"cluster_name\": CLUSTER_NAME},\n",
    "    \"spark_job\": {\n",
    "        \"main_class\": UPSERT_TARGET_MAIN_CLASS,\n",
    "        \"args\": UPSERT_TARGET_CMD_LINE_ARGS,\n",
    "        \"jar_file_uris\": JARS_FILES,\n",
    "        \"properties\": UPSERT_TARGET_SPARK_PROP\n",
    "    },\n",
    "}\n",
    "\n",
    "# DAG Parameters for dynamic configuration\n",
    "dag_params = {\n",
    "    \"machineType\": Param(default=\"n1-standard-4\", type=\"string\", description=\"Machine type for cluster nodes\"),\n",
    "    \"ingDriverCores\": Param(default=\"2\", type=\"string\", description=\"Driver cores for ingestion job\"),\n",
    "    \"ingDriverMemory\": Param(default=\"3g\", type=\"string\", description=\"Driver memory for ingestion job\"),\n",
    "    \"ingExecInstances\": Param(default=\"2\", type=\"string\", description=\"Executor instances for ingestion job\"),\n",
    "    \"ingExecCores\": Param(default=\"2\", type=\"string\", description=\"Executor cores for ingestion job\"),\n",
    "    \"ingExecMemory\": Param(default=\"4g\", type=\"string\", description=\"Executor memory for ingestion job\"),\n",
    "    \"ingMemoryOverhead\": Param(default=\"0.1\", type=\"string\", description=\"Memory overhead factor for ingestion job\"),\n",
    "    \"upsDriverCores\": Param(default=\"2\", type=\"string\", description=\"Driver cores for upsert job\"),\n",
    "    \"upsDriverMemory\": Param(default=\"3g\", type=\"string\", description=\"Driver memory for upsert job\"),\n",
    "    \"upsExecInstances\": Param(default=\"2\", type=\"string\", description=\"Executor instances for upsert job\"),\n",
    "    \"upsExecCores\": Param(default=\"2\", type=\"string\", description=\"Executor cores for upsert job\"),\n",
    "    \"upsExecMemory\": Param(default=\"4g\", type=\"string\", description=\"Executor memory for upsert job\"),\n",
    "    \"upsMemoryOverhead\": Param(default=\"0.1\", type=\"string\", description=\"Memory overhead factor for upsert job\"),\n",
    "    \"clusterType\": Param(default=\"micro\", type=\"string\", decription=\"cluster type for the job\"),\n",
    "    \"numberInstances\": Param(default=\"2\", type=\"string\", description=\"number of instances for job\")\n",
    "}\n",
    "with models.DAG(DAG_ID, tags=TAGS, start_date=pendulum.datetime(2025, 6, 1, tz=\"UTC\"), default_args=default_args, max_active_runs=1, catchup=False, schedule_interval=SCHEDULE, params=dag_params) as dag:\n",
    "\n",
    "    create_cluster = BFDMSDataprocCreateClusterOperator(\n",
    "        task_id='{}_create_cluster'.format(DAG_ID.replace('-','_').lower()),\n",
    "        cluster_name=CLUSTER_NAME,\n",
    "        region=REGION,\n",
    "        project_id=PROJECT_ID,\n",
    "        dpaas_config = CLUSTER_CONFIG,\n",
    "        gcp_conn_id=CONN_ID_DPAAS,\n",
    "        delete_on_error=True\n",
    "    )\n",
    "\n",
    "    icds_ingest_spark_task = DataprocSubmitJobOperator(\n",
    "        task_id='icds_ingest_to_dl',\n",
    "        job=ICDS_INGESTION_SPARK_JOB,\n",
    "        region=REGION,\n",
    "        project_id=PROJECT_ID,\n",
    "        gcp_conn_id=CONN_ID_DPAAS\n",
    "    )\n",
    "\n",
    "    upsert_spark_task = DataprocSubmitJobOperator(\n",
    "        task_id='upsert_dl_target_table',\n",
    "        job=UPSERT_TARGET_SPARK_JOB,\n",
    "        region=REGION,\n",
    "        project_id=PROJECT_ID,\n",
    "        gcp_conn_id=CONN_ID_DPAAS\n",
    "    )\n",
    "\n",
    "    delete_cluster = DataprocDeleteClusterOperator(\n",
    "        task_id='{}_delete_cluster'.format(DAG_ID.replace('-','_').lower()),\n",
    "        cluster_name=CLUSTER_NAME,\n",
    "        project_id=PROJECT_ID,\n",
    "        region=REGION,\n",
    "        trigger_rule='all_done',\n",
    "        gcp_conn_id=CONN_ID_DPAAS\n",
    "    )\n",
    "\n",
    "    end = EmptyOperator(task_id=\"end\")\n",
    "\n",
    "    create_cluster >> icds_ingest_spark_task >> upsert_spark_task >> [delete_cluster, end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486af41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "How to send argument in python file and print the \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "4f591c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Created: /Users/vn59a0h/Desktop/projects/Ingestion/Automation/updated_dag.py\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/Users/vn59a0h/Desktop/projects/Ingestion/Automation/updated_dag.py'"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def prepare_dag_file(sample_dag_file, dag_config, output_file):\n",
    "    \"\"\"\n",
    "    Generate a customized DAG file from template using string replacement\n",
    "    \n",
    "    Args:\n",
    "        sample_dag_file: Path to template DAG file\n",
    "        dag_config: Dictionary with configuration values\n",
    "        output_file: Path to output file\n",
    "    \"\"\"\n",
    "    with open(sample_dag_file, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    \n",
    "    updated_lines = []\n",
    "    for line in lines:\n",
    "        new_line = line\n",
    "        \n",
    "        # Edit1: Replace SENSITIVITY\n",
    "        if line.strip().startswith('SENSITIVITY='):\n",
    "            new_line = f'SENSITIVITY=\"{dag_config[\"sensitivity\"]}\"\\n'\n",
    "        \n",
    "        # Edit2: Replace CLUSTER_NAME\n",
    "        elif line.strip().startswith('CLUSTER_NAME ='):\n",
    "            new_line = f'CLUSTER_NAME = \"{dag_config[\"cluster_name\"]}\"\\n'\n",
    "        \n",
    "        # Edit3: Replace BANNER_NAME\n",
    "        elif line.strip().startswith('BANNER_NAME='):\n",
    "            new_line = f'BANNER_NAME=\"{dag_config[\"banner_name\"]}\"\\n'\n",
    "        \n",
    "        # Edit4: Replace TABLE_NAME\n",
    "        elif line.strip().startswith('TABLE_NAME='):\n",
    "            new_line = f'TABLE_NAME=\"{dag_config[\"table_name\"]}\" #SAP table name\\n'\n",
    "        \n",
    "        # Edit5: Replace TAGS\n",
    "        elif line.strip().startswith('TAGS ='):\n",
    "            tags_str = str(dag_config[\"tags\"]).replace(\"'\", '\"')\n",
    "            new_line = f'TAGS = {tags_str}\\n'\n",
    "        \n",
    "        updated_lines.append(new_line)\n",
    "    \n",
    "    # Write updated content to new file\n",
    "    with open(output_file, 'w') as file:\n",
    "        file.writelines(updated_lines)\n",
    "    \n",
    "    print(f\"✓ Created: {output_file}\")\n",
    "    return output_file\n",
    "\n",
    "\n",
    "# Example usage\n",
    "sample_dag_file = \"/Users/vn59a0h/Desktop/projects/Ingestion/Automation/sample_dag.py\"\n",
    "output_file = \"/Users/vn59a0h/Desktop/projects/Ingestion/Automation/updated_dag.py\"\n",
    "\n",
    "# Configuration dictionary\n",
    "dag_config = {\n",
    "    \"sensitivity\": \"HS\",\n",
    "    \"cluster_name\": \"sa-mdse-dl-secure-mdd-physl-invt-doc\",\n",
    "    \"banner_name\": \"msb\",\n",
    "    \"table_name\": \"ABC\",\n",
    "    \"tags\": [\"Ranjan\", \"P2\", \"Ephemeral\", \"SA\", \"SECURE\", \"MDSE\", \"MDD\", \"mdd_physl_invt_doc\", \"SLT\"]\n",
    "}\n",
    "\n",
    "prepare_dag_file(sample_dag_file, dag_config, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "8fb2554a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing INTLDLDAT-SAMDD-INC-SA_MDSE_DL_SECURE-MDD_PHYSL_INVT_DOC.py  ...\n",
      "Preparing INTLDLDAT-SAMAK-INC-SA_MDSE_DL_SECURE-MAK_PHYSL_INVT_DOC.py  ...\n",
      "Preparing INTLDLDAT-SAMSB-INC-SA_MDSE_DL_SECURE-MSB_PHYSL_INVT_DOC.py  ...\n",
      "Preparing INTLDLDAT-SAMM-INC-SA_MDSE_DL_SECURE-MM_PHYSL_INVT_DOC.py  ...\n",
      "Preparing INTLDLDAT-SAWMT-INC-SA_MDSE_DL_SECURE-WMT_PHYSL_INVT_DOC.py  ...\n"
     ]
    }
   ],
   "source": [
    "# input\n",
    "banner_list = df['BANNER_NAME'].tolist()[0].split(',')\n",
    "tableLoadType = \"INC\"\n",
    "dlSchemaName = \"sa_mdse_dl_secure\"\n",
    "dlTableName = \"PHYSL_INVT_DOC\"\n",
    "\n",
    "for b in banner_list:\n",
    "    dag_name = generate_dag_name(b, tableLoadType, dlSchemaName, dlTableName)\n",
    "    print(f\"Preparing {dag_name}.py  ...\")\n",
    "    # prepare_dag_file(dag_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e3d9e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a72f4d0d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2b80fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
